\chapter{Clustering existing sample points in resource inventories}
\label{chap:Cl_Alg}
{\large Nikolas von Lüpke$^1$ - Marco Bender$^2$ - Jonas Ide$^2$ - Joachim Saborowski$^{1,3}$}\\

\vspace{3cm}
\noindent
$^1$Department of Ecoinformatics, Biometrics and Forest Growth,\\ University of Göttingen, Büsgenweg 4, 37077 Göttingen, Germany \\

\noindent
$^2$Institute for Numerical and Applied Mathematics,\\ University of Göttingen, Lotzestraße 16-18, 37083 Göttingen, Germany\\

\noindent
$^3$Department of Ecosystem Modelling,\\ University of Göttingen, Büsgenweg 4, 37077 Göttingen, Germany\\

%\vspace{\fill}
%\noindent
%Submitted to:\\
%European Journal of Forest Research
\clearpage
\textbf{}
\renewcommand{\labelitemi}{--}
\begin{itemize}
	\item Marco Bender and Jonas Ide developed the optimisation algorithm for the calculation of benchmark solutions and suppoerted the writing of the manuscript.
	\item Joachim Saborowski supported the analysis of the results and the writing of the manuscript.
\end{itemize}

\cleardoublepage
%%%%%%%%%%%%%%
%% Abstract %%
%%%%%%%%%%%%%%
\section*{Abstract}
\label{sec:Cl_Alg:Abstract}
Cluster sampling is a well-known and widely used sampling scheme in resource inventories. Shape and size of the clusters have usually to be determined when setting up a new inventory. In cases where an existing sampling scheme of a previous inventory shall be reduced by subsampling, e.g. for cost reduction or for a fast intermediate inventory, it might be desirable to cluster neighbouring sample points in a way that minimises the within-cluster distances and use clustered subsampling. This is important for achieving high efficiency of terrestrial sampling. Furthermore, it is mostly desirable to generate equally sized clusters that can be sampled within a one-day workload.

The Vehicle Routing Problem (VRP), well-known in Operations Research, can be seen as a similar problem. Its basic idea is that several customers should be supplied from one depot. Therefore, the shortest route covering all customers and fulfilling all customer-demands has to be found. Several heuristics exist for solving that problem.

In a case study in Lower Saxony, Germany, we applied three of these heuristics for building clusters of sample points of the periodic forest district inventory, the (1) Savings-Algorithm, a classical heuristic, and two metaheuristics, (2) a Record-to-Record travel and (3) a Simulated Annealing algorithm. Beyond these VRP-heuristics, we applied (4) a k-means, (5) an equal area partitioning and (6) a hierarchical clustering algorithm, and (7) used the planning unit compartment (Abteilung) as clusters. The results indicate that the VRP-heuristics are well-suited for defining clusters of approximately the same size, whereas the other algorithms produce clusters of highly variable size. Comparing the results with computer intensive benchmark solutions, it could be shown that the total distances are close to the optimal ones.


\subsection*{keywords}
vehicle routing problem - k-means - hierarchical clustering - equal area partitioning - forest inventory - cluster sampling
%%%%%%%%%%%%%%%%%%
%% Introduction %%
%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:Cl_Alg:Introduction}
High efficiency and thus high precision at low inventory-costs are expected from forest inventories. It can be achieved by an adequate sampling-scheme fulfilling the aims of the inventory under the given conditions. Therefore, a multitude of efficient inventory-schemes has been developed over the last decades and still new methods are proposed. Particularly, selection of clustered sampling units can often be a measure to increase efficiency.

When setting up a clustered sampling scheme, mainly two cases are imaginable:
\begin{enumerate}
	\item An inventory is planned with a clustered sampling procedure from the beginning.
	\item The sampling scheme of an existing periodic inventory is to be converted into a clustered sampling scheme with reduced sample size, temporarily or regularly, and with the aim of cost-reduction in mind.
\end{enumerate}

Several studies of the efficiency of different cluster shapes like triangle, cross or square exist \citep[e.g.][]{Kleinn_1994, Kleinn_1996} and it is possible to follow the advices, given in these studies, when setting up a new forest inventory (case 1). In national forest inventories a multitude of different cluster shapes is used \citep{NFI_2010}, and usually the clusters are designed in a way that makes it possible to execute the field work per cluster within one day \citep{Kleinn_1996}. The latter commonly implies homogenous cluster sizes. 

Building such equally sized clusters of existing sample plots (case 2) is especially difficult, when the sample plots are selected by simple random sampling (SRS) and hence the distances between neighbouring sample plots are variable. Studies of the national forest inventories in Switzerland \citep{Zinggeler_1997, Zinggeler_2001} and Germany \citep{Kroiher_2006} showed that the measurements on the plots accounted for 25-40 \% (Switzerland) and 51 \% (Germany) of the total working time. It has to be kept in mind that these numbers are highly influenced by road infrastructure, density of sample plots, plot design, elevation, and slope of the study area. Nevertheless, it becomes obvious that the time-consumption for travelling and indirect inventory-work accounts for a high percentage of the total inventory-time. Therefore, it is desirable to reduce this percentage wherever possible. One approach is to cluster sample plots to daily workloads in a way which minimises the within-cluster distances and thereby the time-consumption for travelling.

An intuitive clustering is to divide the study area into partitions of approximately the same size. This can either be done with special algorithms or by using existing management units such as forest districts (Forstamt), forest sub-districts (Revier) or compartments (Abteilung) as partitions. Another possibility is to use cluster-algorithms, from which a plethora exists \citep{Anderberg_1973, Hartigan_1975}. Well-known and established clustering-methods are k-means and hierarchical algorithms. Whereas in k-means the points are allocated to a given number of clusters with the aim of minimising a target-function, in hierarchical methods points or point-clusters are combined to new clusters in every iteration-step until all points belong to the same cluster. This is done with the aim of combining the most similar clusters in every iteration-step \citep{Anderberg_1973, Hartigan_1975}. These algorithms were not originally developed for spatial clustering, but they can be used for that purpose using the coordinates of the sample points as attributes.

The combination of sample plots to a cluster of spatially contiguous sample points can also be seen as a Vehicle Routing Problem (VRP), which has been introduced by \citet{Dantzig_1959} and is well-known in Operations Research. The basic idea of this problem is that several customers should be supplied from one depot and therefore it is desirable to find the shortest route(s) covering all customers and thereby fulfilling all customer-demands. Up to now heuristic methods are often used in practice for solving VRPs, although exact algorithms exist. This is due to the facts that even the best exact algorithms are able to solve only VRPs with approximately 100 vertices and that quick solutions are expected from the users \citep{Laporte_2009}. The number of possible routes through n vertices can be calculated by $(n-1)!/2$ for the symmetric case, assuming the same distance between two points in both directions. For a problem with 100 points $4.66\cdot10^{155}$ possible solutions exist. Obviously, the calculation of an exact solution would be extremely time-consuming.

Even though the VRP is one of the most famous problems in Distribution Science and one of the most widely studied problems in Combinatorial Optimisation \citep{Cordeau_2002}, applications in Forest Science are still rare. \citet{Thiele_2008} presented results for wood delivery to a fictitious pulp mill, calculated with two classical heuristics for solving this VRP. \citet{Flisberg_2009} combined linear programming and VRP-techniques for routing of logging trucks. Their approach is used in the decision support system for routing of logging trucks RuttOpt \citep{Andersson_2008}.

In the following article we present results that have been obtained with several methods of spatial clustering in a case study with data of the Forest District Inventory of Lower Saxony, Germany. The resulting clusters were evaluated regarding their within-cluster distances and their homogeneity of size; the efficiency of clustered sampling schemes is not part of this study. Obviously, the within-cluster distances should be small for raising the efficiency of an inventory design. 
%%%%%%%%%%%%%
%% Methods %%
%%%%%%%%%%%%%
\section{Methods}
\label{sec:Cl_Alg:Methods}
%%-------------------------------------------%%
%% Algorithms of the Vehicle Routing Problem %%
%%-------------------------------------------%%
\subsection{Algorithms of the Vehicle Routing Problem}
\label{subsec:Cl_Alg:Methods:Algorithms_VRP}
Formally the VRP is defined as follows: $G = \left(V, A \right)$ is a graph with a vertex set $V=\{\nu_0, \nu_1, \dots,\nu_n \}$ and an arc set $A=\{\left(\nu_i,\nu_j \right): \nu_i, \nu_j \in V, i\neq j \}$. The depot is represented by vertex $\nu_0$, the customers by the remaining vertex set $V_{c}=\{\nu_1,\nu_2, \dots,\nu_n \}$. A cost and a travel time matrix are associated with $A$, and often these matrices are assumed to be symmetrical. In such cases the VRP is defined on an undirected graph. A non-negative demand $q_i$ and a service time $t_i$ are associated to every customer $i$. The total number of vehicles $m$ of capacity $Q$ might be known in advance or treated as a decision variable. The whole problem is a generalisation of the Traveling Salesman Problem (TSP), which aims at determining the cheapest round-tour, covering all customers. It arises when $m=1$ and $Q\geq\sum_{i\in V_{c}}q_{i}$  \citep{Baldacci_2010}.

In general, it can be distinguished between (1) exact algorithms, (2) classical heuristics and (3) metaheuristics. Within these categories a high variety of algorithms has been developed, the explanation of which would go beyond the scope of this article. A good introduction to the different approaches can be found in \citet{Laporte_2009}. \citet{Cordeau_2002} compared different heuristics under several aspects and gave advices for the use of these methods.
%%+++++++++++++++++++%%
%% Savings-Algorithm %%
%%+++++++++++++++++++%%
\subsubsection{Savings-Algorithm (CW)}
\label{subsubsec:Cl_Alg:Methods:Algorithms_VRP:Savings}
This algorithm has been developed by \citet{Clarke_1964}, it is well-known and widely used in practice, although its shortcomings are known \citep{Cordeau_2002}. Based on an initial solution of tours which consist of only one vertex, two routes $\left(\nu_0, \dots, \nu_i, \nu_0 \right)$  and $\left(\nu_0, \dots, \nu_j, \nu_0 \right)$ are combined into one $\left(\nu_0, \dots, \nu_i, \nu_j, \dots, \nu_0\right)$ at every iteration step, as long as the restrictions are maintained. Therefore the savings $s_{ij}=c_{i0}+c_{0j}-c_{ij}$ of these point-combinations have to be calculated based on the distance-matrix, where $c_{i0}$ is the cost for the way from $\nu_i$ to $\nu_0$, $c_{0j}$ the cost for the way between $\nu_0$ and $\nu_j$, and $c_{ij}$ the cost for the way from $\nu_i$ to $\nu_j$. The combination of routes which yields the highest saving without violating the restrictions is chosen at every step. Afterwards a post-optimisation is carried out. \citet{Cordeau_2002} attested this method in their comparison a low accuracy and flexibility, but a very high speed and simplicity.
%%+++++++++++++++++++++++++++++++++++%%
%% Record-to-Record travel Algorithm %%
%%+++++++++++++++++++++++++++++++++++%%
\subsubsection{Record-to-Record travel Algorithm (RTR)}
\label{subsubsec:Cl_Alg:Methods:Algorithms_VRP:RTR}
\citet{Dueck_1993} introduced the Record-to-Record travel Algorithm and presented applications to the TSP. During the iteration-process the results do not necessarily need to improve the value of the target function, whose currently best value is called Record. The iteration proceeds even if a new configuration leads to a value worse than the current Record as long as it does not exceed a certain deviation from the latter. \citet{Li_2005} extended this method by a variable-length neighbour list and applied it to very large scale VRPs. Their proposal is provided in the VRPH-library \citep{Groeer_2010}, and we will describe the technique only briefly in the following. An initial solution is generated with a modified Savings-Algorithm. Its value of the target function is used as Record and the Deviation is defined as $1/10$ of this Record. Within a loop the current solution is improved, Record and Deviation are updated afterwards. Post-Optimisation of this solution is carried out, and if an improvement is possible, Record and Deviation are updated again. After repetitions and perturbations the best solution generated so far is kept.
%%+++++++++++++++++++++%%
%% Simulated annealing %%
%%+++++++++++++++++++++%%
\subsubsection{Simulated annealing (SA)}
\label{subsubsec:Cl_Alg:Methods:Algorithms_VRP:Simulated_Annealing}
The first mentioning of Simulated Annealing in the context of Combinatorial Optimisation dates back to 1983, when \citeauthor{Kirkpatrick_1983} proposed its use, based on the algorithm of \citet{Metropolis_1953}. Starting from an initial solution, a new solution is randomly chosen from the neighbourhood of the actual one. Both solutions are compared and the current solution is accepted if it leads to a better value of the target function, or if not, with a probability that decreases with increasing simulation time and growing differences between the values of the target function \citep{Laporte_2009}.
%%-------------------%%
%% K-means algorithm %%
%%-------------------%%
\subsection{K-means algorithm (KM)}
\label{subsec:Cl_Alg:Methods:K_means}
The k-means algorithms belong to the partitioning-algorithms, and in contrast to the VRP-algorithms the number of clusters ($h$) is known in advance and an important input variable for these algorithms. $h$ centroids are initially placed and the sum of the within-cluster sum of squares $J=\sum_{k=1}^h\sum_{j=1}^{m_{k}}\sum_{i=1}^n\left(x_{ijk}-x_{ik} \right)^2$ is calculated, where $m_k$ denotes the number of points in cluster $k$ and $n$ the dimension of the space of variables (Anderberg 1973, Hartigan 1975). Here, $n=2$ and $x$ stands for the coordinates of the sample points. Through shifts of the $m$ points from one cluster to another and recalculation of the centroids in an iteration-process it is desired to minimise $J$. It is this iteration-process that makes the different k-means algorithms distinct from each other. Well-known are those of \citet{Lloyd_1982}, \citet{Forgy_1965}, \citet{MacQueen_1967} and \citet{Hartigan_1979}, from which we applied the latter.
%%-----------------------------------------------%%
%% K-means algorithm for equal area partitioning %%
%%-----------------------------------------------%%
\subsection{K-means algorithm for equal area partitioning (EAP)}
\label{subsec:Cl_Alg:Methods:EAP}
This method has been presented and thoroughly described by \citet{Walvoort_2010} for grid cell data. The spatial data are converted to grid cells and these cells are initially partitioned into clusters. In an iteration process swops of two cells from different clusters are carried out if the sum of the distances between the cells and the centroids of the clusters is shortened through this swop.
%%-------------------------%%
%% Hierarchical clustering %%
%%-------------------------%%
\subsection{Hierarchical clustering (HC)}
\label{subsec:Cl_Alg:Methods:HC}
Whereas the number of clusters is an important input variable in the k-means algorithms, it is not predefined in hierarchical clustering. Here points or point-clusters are combined to a new cluster in every iteration step until all points n belong to the same cluster; thus the number of iteration steps is $n-1$. Different algorithms use different criterions for combining sample points to a cluster \citep{Anderberg_1973}.

Well-known is the method of \citet{Ward_1963}. As in the k-means algorithms, the sum of the within-cluster sum of squares $J$ is used as target-variable. The combination of point-clusters that leads to the smallest increase in $J$ is selected in every iteration-step \citep{Ward_1963, Anderberg_1973}.
%%---------------%%
%% Planning Unit %%
%%---------------%%
\subsection{Planning Unit (PU)}
\label{subsec:Cl_Alg:Methods:PU}
In Germany several different administrative planning units exist, one of the smallest ones is the compartment (Abteilung), on an average being of 18.84 ha in size in the seven forest districts. We used the terrestrial sample plots within each compartment as a cluster.
%%--------------------%%
%% Benchmark Solution %%
%%--------------------%%
\subsection{Benchmark Solution}
\label{subsec:Cl_Alg:Methods:Benchmark_Solution}
As mentioned above heuristics do not guarantee optimality of a solution. To be able to evaluate their quality, we make use of a lexicographic integer programming approach that yields optimal solutions for the considered instances. During this process we only considered feasible tours, i.e. tours that can be finished within one day and thus do not exceed a predefined time-limit per tour.

First, we generate all feasible tours $\mathcal{T}$. For these tours we first solve the integer programme
\begin{xalignat*}{3}
\min  				&\sum_{t \in \mathcal{T}} x_t	&\\
\text{s.t.}		&\sum_{t \in \mathcal{T}} contains[v,t] \; x_t =1  &\text{for all } v \in V \\
							&	x_t \in \{0,1\}																 &\text{for all } t \in \mathcal{T},\\
\end{xalignat*}
which minimises the number of tours necessary to cover all vertices. 

Here, we have a binary decision variable $x_t$ associated with each tour $t \in \mathcal{T}$ that is 1 if tour $t$ is chosen, and 0 else. The parameter $contains[v,t]$ is 1 if tour $t \in \mathcal{T}$ contains vertex $v \in V$, and 0 else, and therefore the constraints ensure that every vertex is visited exactly once.

We denote by $z^*$ the optimal objective value of the programme above. Among all of those cluster partitions we then want to choose a combination of $z^*$ tours that minimise the total distances that need to be travelled. If we denote by $length[t]$ the length of tour $t \in \mathcal{T}$, this can be done by solving the integer programme
\begin{xalignat*}{3}
\min  				&\sum_{t \in \mathcal{T}} length[t] \; x_t				&\\
\text{s.t.}		&\sum_{t \in \mathcal{T}} contains[v,t] \; x_t =1 & \text{for all } v \in V \\
							&\sum_{t \in \mathcal{T}} x_t = z^* 						& \\
							&	x_t \in \{0,1\}																	& \text{for all } t \in \mathcal{T}.\\
\end{xalignat*}
%%%%%%%%%%%%%%%%
%% Case study %%
%%%%%%%%%%%%%%%%
\section{Case study}
\label{sec:Cl_Alg:Case_study}
%%-----------%%
%% Data base %%
%%-----------%%
\subsection{Data base}
\label{subsec:Cl_Alg:Case_study:Data_base}
For the case study we used data from the Forest District Inventory (Betriebsinventur) of Lower Saxony, Germany. This inventory is carried out according to a double sampling for stratification scheme since 1999 in a cycle of about ten years \citep{Boeckmann_1998}. Sample points are located in a 100 m $\times$ 100 m grid within the first phase of this procedure. Depending on dominating species group (DEC: Deciduous; CON: Coniferous) and age class (1: $\leq$ 40 years; 2: 41-80 years; 3: 81-120 years; 4: $>$ 120 years) every point is assigned to one of eight strata. In the second phase a certain proportion of sample points, differing among the strata, is chosen in each stratum. The selection is done systematically from a list of first-phase units per stratum, which however leads to an irregular (random-like) spatial  distribution of second-phase units on the first-phase grid. 

Two concentric plots (6 m radius for trees with 7 cm $\leq$ dbh $<$ 30 cm and 13 m radius for trees with dbh $\geq$ 30 cm) are established and inventoried at these points. The first run of this inventory has been carried out between 1999 and 2008, and we used the sample points of seven forest districts (Forstämter) in the regions Harz and Solling (Figure \ref{fig:Cl_Alg:figure_1}). Meanwhile merges of forest districts have been executed (see www.landesforsten.de), so that not all of those forest districts still exist in the shape underlying our study. As calculation units for the clustering we used the 70 forest sub-districts (Reviere), each of which contains between 120 and 514 sample points. 

\begin{figure}
\begin{center}
  \includegraphics{Grafiken/Cl_Alg/figure_1.pdf}
\caption{The seven Forest Districts analysed in the case study.}
\label{fig:Cl_Alg:figure_1}
\end{center}
\end{figure}
%%-----------------%%
%% Data processing %%
%%-----------------%%
\subsection{Data processing}
\label{subsec:Cl_Alg:Case_study:Data_processing}
%%++++++++++%%
%% Software %%
%%++++++++++%%
\subsubsection{Software}
\label{subsubsec:Cl_Alg:Case_study:Data_processing:Software}
For solving the VRP we used the VRPH-library of \citet{Groeer_2010}. In this open access library several algorithms are implemented, from which we applied the three algorithms mentioned earlier. Both, pre-processing of the data and evaluation of the VRPH-results, were done with the statistical software package R \citep{R_Doc}. The R-package "spcosa" \citep{Walvoort_2010, Walvoort_2012} was used for the modified k-means algorithms for equal area partitioning. 
For calculation of the benchmark solution, the generation of the tours has been implemented in C++. The integer programme modelling was performed in IBM ILOG OPL 6.3 and as solver we used IBM ILOG CPLEX 12.4.
%%++++++++++++++++++++++++++++%%
%% Pre-processing of the data %%
%%++++++++++++++++++++++++++++%%
\subsubsection{Pre-processing of the data}
\label{subsubsec:Cl_Alg:Case_study:Data_processing:Pre_processing}
Originating from their original purpose in logistics, all VRP-heuristics need a load, which the truck should deliver or pick up at each vertex, and a capacity of the trucks. In our application we set the load at every sample point to 1 and the capacity to 5. This assumption is based on the observations of \citet{Zinggeler_1997} and \citet{Zinggeler_2001}, who gave a per plot working time for the terrestrial inventory of 179 minutes for one person during the Second Swiss National Forest Inventory. Due to the fact that the sampling scheme of this inventory is similar to the one used in the Forest District Inventory of Lower Saxony \citep{Boeckmann_1998, Stierlin_2001}, we assumed a two-people inventory team to sample on average five sample plots per day. We calculated results for ten randomly distributed starting points for a sub-district, because these algorithms need a starting point, which is not naturally and uniquely defined in our study. These starting points were distributed in a rectangle, defined by the ranges of a sub-district in east-west and south-north direction. The side-lengths of the rectangle were 1.25 times the respective range.

For KM, EAP and HC the number of clusters has to be defined in advance of the iterations. For the three techniques we calculated the number of clusters by $n/5$ and rounding up to the next integer (function "ceil") if necessary.

Whereas it was possible to use the Euclidean Distances between the sample points in all but one algorithm, it was necessary to convert the point data to pixel data for the application of EAP \citep{Walvoort_2010}. Thus, the shape-files were converted into grid cell data with a cell size of 50 m.

During the calculation of the benchmark solution only feasible tours should be considered. Therefore, it was necessary to assess the time consumption for the inventory on the plot and for walking between the plots. As for the VRP-algorithms, we assumed a working time of 1.5 h per plot, like in the Second Swiss National Forest Inventory \citep{Zinggeler_1997, Zinggeler_2001}. For walking between the plots we presumed a walking speed of 3.33 km/h, as given by \citet{Scott_1993} for medium terrain.
%%++++++++++++++++++++++++++++++%%
%% Post-processing of the tours %%
%%++++++++++++++++++++++++++++++%%
\subsubsection{Post-processing of the tours}
\label{subsubsec:Cl_Alg:Case_study:Data_processing:Post_processing}
First of all we built a circle tour, in which the last point $\nu_n$ is connected to the first point of the tour $\nu_1$. Afterwards we further optimised this tour with a 2-opt Algorithm (Figure \ref{fig:Cl_Alg:figure_2}) \citep[R library "TSP",][]{Hahsler_2007, Hahsler_2011}.
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{Grafiken/Cl_Alg/figure_2.pdf}
\caption{Post-Optimisation of the generated tours with help of a 2-opt algorithm. The start- and endpoint of the initial solution (a) are connected (b). Finally, two edges of the tour are systematically exchanged (c) until the tour could not further be improved.}
\label{fig:Cl_Alg:figure_2}
\end{center}
\end{figure}
%%++++++++++++++++++++++++%%
%% Evaluation of the data %%
%%++++++++++++++++++++++++%%
\subsubsection{Evaluation of the data}
\label{subsubsec:Cl_Alg:Case_study:Data_processing:Evaluation}
With this case study we tried to figure out the performance of the seven different clustering methods and therefore calculated the total within-cluster distance and the number of sample plots per cluster. For the comparison of the methods we used the best as well as the worst result of VRP-algorithms in each forest district, in terms of total distances, achieved using the different starting points. Multiple comparisons were done with pairwise, paired Wilcoxon comparisons with adjustment of p-values after Bonferroni and a significance level of 5 \%.
%%---------%%
%% Results %%
%%---------%%
\subsection{Results}
\label{subsec:Cl_Alg:Case_study:Results}
Already a rapid visible inspection of Figure \ref{fig:Cl_Alg:figure_3} shows that the different algorithms lead to different results in terms of different tour-lengths and -sizes. Obviously, the three VRP-algorithms generate clusters of approximately the same size, whereas the resulting cluster-sizes of the three other algorithms and the PUs are highly variable.
\begin{figure}
\begin{center}
  \includegraphics[height=0.85\textheight]{Grafiken/Cl_Alg/figure_3.pdf}
\caption{Clusters of sub-district 11 in the Forest District Winnefeld. The results were generated with three VRP-heuristics (CW: Savings, RTR: Record-to-Record Travel, SA: Simulated Annealing), two classical clustering algorithms (KM: K-means, HC: Hierarchical Clustering), an equal area partitioning algorithm (EAP), and by using the planning unit compartment (PU).}
\label{fig:Cl_Alg:figure_3}
\end{center}
\end{figure}
It can be seen from Figure \ref{fig:Cl_Alg:figure_4}a that the VRP-algorithms and the EAP lead to nearly the same mean tour length of approx. 1.5 km in the 70 forest sub-districts. The three other methods lead to shorter mean tour lengths. Significant differences could not be detected only between CW\textsubscript{Min} and SA\textsubscript{Min} and between CW\textsubscript{Max} and SA\textsubscript{Max}.
\begin{figure}
\begin{center}
  \includegraphics[height=0.75\textheight]{Grafiken/Cl_Alg/figure_4.pdf}
\caption{Key figures of the tours of the 70 forest sub-districts. The tours were calculated with seven different algorithms. From the three VRP-heuristics CW (Savings), RTR (Record-to-Record) and SA (Simulated Annealing) the best (Min) as well as the worst (Max) solution of the 10 starting points was used. Moreover, hierarchical clustering (HC), k-means (KM), equal area partitioning (EAP) and using compartments as clusters (PU) were applied. a) mean tour length (km), b) Coefficient of Variation (\%) of the number of plots per tour, c) proportion (\%) of tours with 5 sample plots, d) relative deviation (\%) of the overall length from the overall length of the benchmark solution. }
\label{fig:Cl_Alg:figure_4}
\end{center}
\end{figure}
The differences between the shortest and the longest total tour length per sub-district of the VRP-algorithms, caused by different starting points, are mostly small (Figure \ref{fig:Cl_Alg:figure_5}). The longest sub-district tour is mostly less than 10 km longer than the shortest sub-district tour. These absolute values correspond to relative differences of about 10 \%, related to the minimal total tour length per sub-district. The mean range of RTR is significantly different from that of CW and SA.
\begin{figure}
\begin{center}
  \includegraphics{Grafiken/Cl_Alg/figure_5.pdf}
\caption{Range between the total tour distances per sub-district of the worst and the best solution. The absolute values (km) as well as the relative values (\%), relating to the best solution, are shown for the three VRP-heuristics CW (Savings), RTR (Record-to-Record) and SA (Simulated Annealing). 10 starting points were used for every algorithm and forest district.}
\label{fig:Cl_Alg:figure_5}
\end{center}
\end{figure}
One aim of clustering was to achieve clusters of approximately the same size. Therefore, we looked at the Coefficient of Variation (CV) of the number of sample points per tour in every forest district and the percentage of clusters that contain the desired number of sample points (5 in our case study) (Figure \ref{fig:Cl_Alg:figure_4}b, c). It is evident that the VRP-algorithms are well suited for generating clusters of the same size. The CV is low and the percentage of plots with the desired size is high, even for the worst solutions in terms of overall tour length per sub-district. In contrast HC, KM, EAP and PU naturally generate clusters of variable size. This can be seen from the low percentage of tours with 5 sampling points and the high CV. 

Compared to the (optimal) benchmark solution (Figure \ref{fig:Cl_Alg:figure_4}d), the VRP-algorithms and EAP generate longer overall distances per sub-district, whereas the three other algorithms lead to even shorter sub-district distances. The latter is possible, because the benchmark solution obeys the maximal cluster size constraint. Significant differences could not be detected between CW\textsubscript{Max} and SA\textsubscript{Max} and between CW\textsubscript{Min} and SA\textsubscript{Min}.
%%----------------------------%%
%% Discussion and Conclusions %%
%%----------------------------%%
\subsection{Discussion and Conclusions}
\label{subsec:Cl_Alg:Case_study:Discussion}
Obviously, only the VRP-algorithms are suitable tools for generating clusters of approximately the same size with short overall distances, even though the resulting tour-lengths are longer than those generated with the other clustering-techniques. Homogenous clusters are generated independently from the starting point with the VRP-heuristics. 

The hierarchical clustering is not appropriate for this purpose, because clusters of different sizes result. This is due to the fact that the algorithm tries to minimise the overall sum of the within-cluster sum of squares. Within this process the cluster-size is irrelevant. The same holds for the k-means algorithm, where the number of clusters is pre-defined instead of the cluster-size. By minimising the sum of the within-cluster sum of squares the cluster-size is again disregarded. This leads to smaller overall distances but, seen as a drawback in our context, to highly variable cluster-sizes. 

The equal area partitioning aims at dividing the study area into partitions of approximately the same size. Thus, all the resulting clusters represent nearly the same area, but not the same number of sample points, because the sampling density varies within a sub-district. Also the use of the compartments as clusters leads to highly variable cluster-sizes. Using the planning units as clusters yields also only small mean deviations from the (optimal) benchmark solution, but shows the absolutely highest CV of cluster sizes.

With mean deviations of less than 15 \% from the benchmark solution, the best VRP-solutions are close to the optimal solution. Unfortunately, some few starting points lead to inacceptable solutions with deviations of more than 50 \%. Regarding the benchmark solutions, it has to be kept in mind that a time- and thus distance limit for every tour exist. Such a limit does not exist within the VRP-heuristics; rather tours of five sample plots are built. In contrast to the benchmark solution, tours with less than five plots are exceptional. Hence, the resulting within-cluster and overall distances are longer. This form of clustering becomes especially disadvantageous in cases where spatial clusters of sample points exist. In such cases sample points of different spatial clusters will be combined to a tour resulting in larger tour lengths.

The comparison of the three different VRP-heuristics shows that the RTR-algorithm seems to be the preferable heuristic. It yields the smallest deviation from the benchmark solution and the smallest range between the worst and best solution, generated with different starting points. This dependence of the results from the starting point is one of the main problems of using the VRP-heuristics for clustering, even though the differences between the two extreme values are mostly small. For practical applications the position of the administration or the central office could be used as a starting point or several runs could be executed and the best result used. Due to the short computation-times, the latter is easily possible.  In cases where more than one starting point exists, it should be possible to solve the routing-problem with one of the algorithms for the Multi Depot Vehicle Routing Problem (MDVRP). Another possibility would be to split the area of interest into several sub-units and apply the VRP-algorithms therein afterwards.

An advantage of the VRP-heuristics is that they can be applied even for a high number of sample points, where an optimal solution cannot be calculated at all or at least in an acceptable time. With help of the VRPH-library \citep{Groeer_2010} they are easy to implement and can be calculated fast. 

Even though we consider three of the presented heuristics as suitable tools, their use is not free of problems and the results have to be questioned. Thus it is questionable, if we have chosen a realistic calculation unit when deciding for forest sub-districts. A sub-district is a human-made planning unit, the shape of which is not completely dependent on spatial reasons. Thus, closely neighbouring sample points might belong to different sub-districts. Nevertheless, the sub-districts are a meaningful calculation unit. The number of sample points within them is manageable for the presented heuristics and it makes sense to plan the execution of the inventory in accordance with existing planning units.

Clustering is usually aimed at achieving a high variation within the clusters and a low variation between them. Due to the fact that neighbouring sample plots often fall into the same or similar stand type, the fulfilment of this request cannot be ensured with the algorithms. To increase variability within clusters, the clusters would have to be enlarged to an extent that exceeds the daily workload of an inventory team, or spatial contiguity would have to be disregarded, in contradiction to the requirement of short sample point distances within clusters.
Unfortunately, we could not use exact distances between the terrestrial sample plots in our calculations. This is due to the fact, that only values of easting and northing and not of the elevation were available. We used Euclidean Distances as proxies and are aware of the facts that this leads to biased results and that the real distances are longer. In our application the distances between sample points are short in most cases - with a mean tour length of 1.5 km and a tour of 5 sample points approximately 300 m (Figure \ref{fig:Cl_Alg:figure_4}) - and so the differences between the real and the Euclidean Distances will likely be negligible. Moreover, the use of these algorithms might help to estimate the resources needed for executing the inventory, which is interesting for the responsible authorities. The generated clusters can be used as suggestions for the execution of the inventory or as clusters in further clustered sub-sampling. Due to the large number of sample points in the Forest sub-districts, the use of heuristics seems to be a good choice for calculating solutions in an acceptable time.
%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements %%
%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
\label{sec:Cl_Alg:Acknowledgements}
We thank the German Science Foundation (DFG) for financial support of this study (Sachbeihilfe SA 415/5-1) and Dr. T. Böckmann of the Lower Saxony Forest Planning Office for his kind provision of the inventory data.




%\bibliographystyle{apalike2}
%\bibliography{Dissertation}