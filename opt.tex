\chapter{Flexible Global Optimization with Simulated-Annealing}
\label{chap:opt}
{\large Kai Husmann$^1$, Alexander Lange$^2$, Elmar Spiegel$^3$}\\

\vspace{3cm}
\noindent
$^1$University of Göttingen\\Department of Forest Economics and Forest Management, Büsgenweg 3, 37077 Göttingen, Germany \\

\noindent
$^2$University of Göttingen\\Chair of Econometrics, Humboldtallee 3, 37073 Göttingen, Germany\\


%\vspace{0.5cm}
\noindent
$^3$University of Göttingen\\Chair of Statistics, Humboldtallee 3, 37073 Göttingen, Germany\\

\vspace{\fill}
\noindent
Submitted to:\\
\textit{The R Journal.}

\newpage
\renewcommand{\labelitemi}{--}
\begin{itemize}
	\item Alexander Lange is co-author of the R package and performed the SVAR example.
	\item Elmar Spiegel reviewed the code of the package, supported writing of the manuscript and the review process.
\end{itemize}

\clearpage
%%%%%%%%%%%%%%
%% Abstract %%
%%%%%%%%%%%%%%
\section*{Abstract}
\label{chap:opt:Abstract}
Standard numerical optimization approaches require several restrictions. So do exact optimization methods such as the Linear Programming approach appeal for linearity and Nelder-Mead for unimodality of the loss function. One method to relax these assumptions is the Simulated Annealing approach, which reduces the risk of getting trapped in a local optimum. However, the standard implementation still requires regular parameter spaces and continuous loss functions. To address this issue, we implemented a version of the Simulated Annealing method that is able to deal with irregular and complex parameter spaces as well as with non-continuous and sophisticated loss functions. Moreover, in order to gain fast but reliable solutions, we included steps to shrink the parameter space during the iterations. All these steps are summarized in the R package \textit{optimization}, which we will introduce in the following article. We also included generic and real world applications in order to test our approach.

%%%%%%%%%%%%%%%%%%
%% Introduction %%
%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:opt:Introduction}
As early computer-based optimization methods developed simultaneously with the first digital computers \citep{corana_1987}, numerous optimization methods for various purposes are available today \citep{wegener_2005}. One of the main challenges in Operations Research is therefore to match the optimization problem with a reasonable method. The complexity of the problem determines possible methods. Optimization procedures in general can be distinguished into exact methods and heuristics \citep{kirkpatrick_1983}. For simple optimization problems, exact methods are often meaningful tools of choice. If all assumptions on model loss and restrictions are met, these methods will obligatorily find the exact solution without need for further parameters. They are the easiest way of solving optimization problems. The \textit{Linear Simplex Method} \citep{dantzig_1959} is one example that only needs the loss-function and optional restrictions as model input. If, however, any of the model assumptions, e.g. linearity, is violated, exact methods are unable to solve problems in a valid way. With developing computer power, heuristics like the \textit{Savings-Algorithm} \citep{clarke_1964} and metaheuristics like \textit{Simulated Annealing} (SA) \citep{kirkpatrick_1983} became popular. They enable solving more complex optimization problems. Metaheuristics are a generalization of heuristics with aim to be even more flexible and efficient \citep{blum_2003} so that they can solve complex optimization problems, such as nonlinear problems. Direct search methods like the \textit{Nelder-Mead} (NM) algorithm are comparatively efficient methods which directly converge to the functions optimum and need relatively few settings \citep{geiger_1999}. Random search methods are able to cope with multimodal objective functions. On the one hand, depending on the method of choice, more or fewer assumptions on the loss function can be neglected. On the other hand, heuristics and metaheuristics will always solve problems approximately. Precision of the solution depends on the optimization method and further parameters. There is even no guarantee of approximating the actual optimum since the solution also depends, contrary to exact methods, on parameterizations \citep{blum_2003}. Defining proper parameters is thus a crucial point of those methods. The complexity of parameterization will by trend increase with the flexibility of the method while efficiency tends to decrease. The efficiency and accuracy of such models is strongly sensitive to their parameter specification \citep{corana_1987}. Heuristics are often programmed for multi-purpose usage such that there is a suitable method for many optimization problems. For complex optimization problems, however, multi-purpose optimizers often fail to find solutions. Additionally, multi-purpose optimizers are usually not suitable or not efficient for highly complex problems like problems with restricted parameter space. Whenever general-purpose optimizers are too restrictive or inflexible to solve a problem properly, specific methods are advantageous. They offer many variable parameters and can thus be parameterized in a way that is very specific to the optimization problem. They represent the most flexible and the most complex optimization methods \citep{blum_2003}.

SA \citep{kirkpatrick_1983} is known to be one of the oldest and most flexible metaheuristic methods, though the term metaheuristic was established after publication of SA \citep{blum_2003}. It is known to be better suited to multimodal loss functions with a very high number of covariates than many other methods \citep{corana_1987}. The method has been applied in many studies of several fields such as chemistry \citep{agostini_2006}, econometrics \citep{ingber_1993} or forest sciences \citep{baskent_2002, boston_1999}. Since its first implementation by \citet{kirkpatrick_1983}, many authors have modified the algorithm in order to adopt it for specific problems \citep[e.g.][]{desarbo_1989, goffe_1996} or more general applications \citep[e.g.][]{xiang_2013}. It combines systematic and stochastic components, and thus enables escaping local optima. It is hence typically used for global optimization of multimodal functions. As it offers many options, SA can be seen a hybrid method between a general optimizer (when default values are chosen) and a problem specific optimization algorithm \citep{wegener_2005}. \citet{corana_1987} developed a dynamic adoption method for the variation of the stochastic component during the optimization process. Their modification affects the efficiency as well as the accuracy of the SA algorithm. It has potential to substantially improve the method. \citet{pronzato_1984} suggests decreasing the search-domain of the stochastic component with increasing number of iterations. The stochastic component in general is the most sensitive part of the method since it determines the loss variables modification during the iterations.

The R software environment provides a platform for the simple and effective distribution of statistical models to a large user community \citep{xiang_2013}. Thus, not surprisingly, several optimization packages of high quality can currently be purchased via \textit{Comprehensive R Archive Network} \citep{theussl_2016}, where even the SA method is recently listed five times. However, we believe that there is need for a specific stochastic optimization package for complex optimization problems. A package coping with very flexible user definable loss functions with multiple options could be an advantageous extension for R. There is demand for specifically definable optimization problems. We therefore present the package \textit{optimization} which is basically a modified version of SA. We used the properties of SA to program a stochastic optimization method for specific purposes. We therefore focused on flexibility and implemented many user specifiable parameters. Our method is, in its default configuration, usually not immediately efficient but flexibly adoptable to specific purposes. The main advantages of the package are the possibilities to specifically adjust covariate changing rules as well as the robustness of the loss function. For example, the changing rule allows the user to define an integer parameter space. The loss function can return any value; even \texttt{NA} or \texttt{NaN} are possible. Several further user specifications help to parameterize the model in a problem-specific way, so that the user can influence accuracy and speed in very detailed ways. It is also the first R function where the improvements of \citet{corana_1987} and \citet{pronzato_1984} are implemented into an SA based optimization software. This means that the search domain of the stochastic component of SA dynamically shrinks with increasing iterations. We also implemented a generic plot function for post-hoc inspection of the model convergence assessment and the solution quality.

In the following, we briefly introduce the algorithm methodologically and explain the most relevant parameters. We show in four examples in which ways our model is favorable against standard methods and explain how it can be parameterized. We develop two examples illustrating the basic model behavior with a focus on the covariate changing rule. Additionally, we include suggestions on how to adopt the numerous options to specific problems. Two practical examples where our function is recently used underpin the relevance of our specific-purpose optimization method. One of them, the optimization of forest harvesting schedules, is a relatively complex example which cannot be solved with any other optimization function in the R framework.

%%%%%%%%%%%%%
%% Package %%
%%%%%%%%%%%%%
\section{The package optimization}
\label{sec:opt:package}
In this section, we explain the theory of our SA interpretation and the resulting parameters.

\subsection{Method}
\label{subsec:opt:package:method}
Since the basic idea of classic SA is derived from the physical process of metal annealing, the nomenclature of SA comes particularly from metallurgy. Just as the classic SA, our function is composed of an inner for loop and an outer while loop \citep{kirkpatrick_1983}. The number of iterations in both loops can be defined by the user. To enhance performance, the loops are written in C++ using \textit{Rcpp} \citep{eddelbuettel_2013}. For better overview, we displayed the important steps of our function in a pseudocode (Algorithm~\ref{alg:opt:alg1}).

\subsubsection{Inner loop}
The function of the inner for loop (Algorithm~\ref{alg:opt:alg1}, lines 4 to 29) is to draw covariate combinations stochastically and to compare the returns. The loop repeats $n_{inner}$ times. The first operation of the inner loop (line 5) is saving the covariate combinations of the last inner iteration as $x_j$. In the first iteration $x_j$ is the vector with user defined initial covariates.

In the second step (line 6), the covariates are changed. This changing process marks an essential difference between our approach and the other SA based method in the R framework. The shape and behavior of the variation process may be defined by the user and may be dynamic. Suggestions and examples for specifying this variation, which is a user defined R function, will be given in the following sections. The variation function \textit{vf} is used to create a temporary vector of covariates $x_{i*}$. Besides the former covariate combination $x_j$, the variation function can depend on a vector with random factors \textit{rf} and the temperature \textit{t}. Since \textit{rf} and \textit{t} change over time, the variation function can have dynamic components. Adjustment of \textit{rf} and \textit{t} is done in the outer loop which will be explained explicitly in the following subsection. In the classical SA approach, the covariates $x_{i*}$ are generated by adding or subtracting a uniformly distributed random number to $x_j$ \citep{kirkpatrick_1983}. The range of the uniform random number is, in our function, determined by \textit{rf} whereas \textit{rf} is relative to $x_j$. A random factor of 0.1 and a covariate expression of three e.g. leads to a uniform random number between 2.7 and 3.3. This standard variation function is also default in our approach. A very simple exemplary modification of the variation function could be a normally distributed random number with mean $x_j$ and standard deviation \textit{rf}.

\SetAlCapSkip{2ex}
\begin{algorithm}[h]
	initialize \textit{t}, \textit{vf} with user specifications\\
	calculate $f(x_0)$ with initial parameter vector $x_0$\\
	\While{\textit{t} $>$ $t_{min}$}{
		\For{i in 1: $n_{inner}$}{
			$x_j \gets x_{i-1}$\\
			call the variation function to generate $x_{i*}$ in dependence of $x_{j}$, \textit{rf} and \textit{t}\\
			check if all entries in $x_{i*}$ are within the boundaries\\
			\eIf{all $x_i$ valid}{calculate $f(x_{i*})$}{
				\While{any($x_{i*}$ invalid)}{
					call the variation function again\\
					count and store invalid combinations}
			}
			\eIf{$f(x_{i*}) < f(x_j)$}{
				$x_{i} \gets x_{i*}; f(x_{i}) \gets f(x_{i*})$
			}{calculate Metropolis Probability \textit{M} (Equation~\ref{eq:opt:eq1})\\ \eIf{uniformly distributed random number [0,1] $<$ \textit{M}}{$x_{i} \gets x_{i*}; f(x_{i}) \gets f(x_{i*})$}{$x_{i} \gets x_j; f(x_{i}) \gets f(x_j)$}}
			\If{threshold accepting criterion fulfilled}{break inner loop}
		}
		
		{
			reduce \textit{t} for the next iteration\\
			\textit{rf} adaptation for the next iteration\\
		}
	}
	\textbf{return} optimized parameter vector, function value and some additional information \\
	\caption{Pseudocode of the \texttt{optim\_sa} function in the optimization package exemplary for a minimization.}
	\label{alg:opt:alg1}
\end{algorithm}

After generating $x_{i*}$, the boundaries are checked (lines 7 to 15). If all entries in $x_{i*}$ are within their respective boundaries, the response is calculated. Otherwise the invalid entries of $x_{i*}$ are drawn again until all entries are valid. According to \citet{corana_1987}, the number of invalid trials can be useful information in order to assess the quality of the search domain. The numbers of invalid trials are thus counted and stored (line 13) in order to make this information accessible for the outer loop. The count of valid and invalid trials is not reset after each inner loop repetition. Both are only initialized in iteration one of the inner loop and increase until the last iteration.

Next step is the comparison of loss function returns (lines 16 and 17). If the return of current variables combination $f(x_{i*})$ is better than $f(x_j)$, $x_{i*}$ and $f(x_{i*})$ are stored into $x_{i}$ and $f(x_{i})$, so $x_{i}$ are the initial covariates for the next iteration. The core idea of the classical SA approach is to cope with the problem of local optima. Thus, even if $f(x_{i*})$ is worse than $f(x_{j})$, there is a chance of storing $x_{i*}$ into $x_i$ (lines 18 to 25). The likelihood of keeping worse responses depends on the Metropolis probability \textit{M} \citep{metropolis_1953}.
\begin{equation}
\label{eq:opt:eq1}
M = exp \left(-\frac{\mid f(i_*)-f(j)\mid}{kt}\right),
\end{equation}
% No line between eq. and text
with \textit{k} being a user definable constant. We adopted this strategy from classic SA without modifications. \textit{M} decreases with decreasing temperature \textit{t} (Equation \ref{eq:opt:eq1}). The likelihood of keeping worse responses thus depends on the same parameters for the whole inner loop (lines 19 and 24). \textit{t} does not change during the entire inner loop. The likelihood of keeping worse values is thus equal for each response until the inner loop is completed. Modification of \textit{t} is part of the outer loop which will be explained in the next paragraph. If a worse result is chosen the former optimal covariate combination is, of course, stored before it is overwritten since otherwise there is a sound chance of overwriting the actual global optimum. More details of the Metropolis probability can i.a. be found in \citet{kirkpatrick_1983} and \citet{metropolis_1953}.

Storing the information on development of covariates and response can help improving the performance of SA \citep{lin_1995, hansen_2012}. We implemented a threshold accepting strategy \citep{dueck_1990} into our SA interpretation (lines 26 to 28). This criterion is the only module that allows reducing the inner loop repetitions without direct user influence. It is simply a vector where the absolute differences of $f(x_{i})$ and $f(x_j)$ are stored. If the response oscillates for a user defined number of repetitions within a user defined threshold, the inner loop breaks.

\subsubsection{Outer loop}
The main functions of the outer while loop (Algorithm \ref{alg:opt:alg1}, lines 3 to 32) are calling the inner loop (lines 4 to 29) and modifying the parameters that are needed in the inner loop (lines 30 and 31). Therefore \textit{t} and \textit{rf} only change after completely finishing an inner loop. The outer loop repeats until \textit{t} is smaller than the user defined minimum temperature $t_0$.

After finishing the inner loop, firstly \textit{t} is adjusted (line 30). \textit{t} is necessary for the stochastic part in the inner loop (line 19, Equation~\ref{eq:opt:eq1}). In our function, \textit{t} decreases per definition as it is calculated by multiplying the temperature of the current iteration by \textit{r} which is a user defined real number between 0 and 1. The number of outer loop repetitions is thus implied by initial temperature $t_0$, $t_{min}$ and \textit{r}.

Afterwards \textit{rf} changes (line 31). The dynamic adaption of \textit{rf} after \citet{corana_1987} and \citet{pronzato_1984} is another major novelty of our function. As each covariate can have its own random factor, \textit{rf} is a vector of the same size as $x_i$. \textit{rf} is needed for the covariate variation in the inner loop (lines 6 and 12). Dividing the numbers of invalid trials distinctively for each covariate by the total number of trials of the respective covariate gives the ratio of invalid trials for each covariate. The numbers of invalid trials are counted in line 13. According to \citet{corana_1987}, this ratio of invalid trials can be used to find a trade-off between accuracy and the size of the search domain. They argue that if only valid covariate combinations are drawn, the search domain could be too small for multimodal problems. For this, the ratio of invalid trials in the current iteration is used to generate the \textit{rf} for the following outer loop repetition. They suggest ratios between 0.4 and 0.6. If any observed ratio of invalid trials is < 0.4 or > 0.6, the respective random factors are modified following the suggested equation by \citet{corana_1987}. This strategy allows an adjustment of \textit{rf} for the next iteration. \citet{pronzato_1984} who developed the \textit{Adaptive Random Search method}, propose a time decreasing search domain. Thus they suggest a search domain adjustment that does not depend on former information, as \citet{corana_1987} did, but on the number of iterations. They argue that the search domain should be wide at the beginning to give the algorithm the chance to cope with local optima, and small in the end to allow higher precisions. Later iterations thus require smaller search domains than earlier iterations. We integrated the idea of \citet{pronzato_1984} into the dynamic search domain adjustment of \citep{corana_1987} by linearly shrinking the favorable range of ratios from the suggested ratios from \{0.4, 0.6\} to \{0.04, 0.06\}. As mentioned in the inner loop explanations, the variation function can be user defined (lines 6 and 12). The user thus has the option to define flexibly in which way \textit{t} and \textit{rf} influence the covariate variation. Per default, the search domain around the covariates shrinks by trend as the number of outer loop iterations increases.
%%-----------------------%%
%% The function optim_sa %%
%%-----------------------%%

\subsection{The function optim\_sa}

\label{subsec:opt:package:fun}
As \texttt{optim\_sa} shall be able to solve very specific optimization problems, several parameters can be defined by the user. The quality of solution and speed of convergence will thus substantially depend on accurate parametrization. In the following, we will explain the most important parameters briefly and make suggestions for useful specification. A complete parameter list can be found in the vignette of the optimization package \citep{husmann_2017}.
\begin{itemize}
	\item \texttt{fun}: Loss function to be optimized. The statement is without default. The function must depend on a vector of covariates and return one numeric value. There are no assumptions on covariates and return. The covariates do not even need to be continuous. Missing (\texttt{NA}) or undefined (\texttt{NaN}) returns are also allowed. Any restriction on the parameter space, e.g. specific invalid covariate values within the boundaries, can be directly integrated into the loss function by simply returning \texttt{NA}. We will include more specific information on this in the practical examples.
	\item \texttt{start}: Numeric vector with initial covariate combination. This statement has no default. It must be ensured that at least the initial covariate combination leads to a defined numeric response. The loss function at the initial variables combination must therefore return a defined numeric value. This might be relevant when the starting values are determined stochastically.
	\item \texttt{trace}: If \texttt{TRUE}, the last inner loop iteration of each outer loop iteration is stored as a row in the trace matrix. This might help evaluating the solutions quality. However, storing interim results increases calculation time by up to 10 \%. Disabling \texttt{trace} can thus improve efficiency when the convergence of an optimization problem is known to be stable.
	\item \texttt{lower, upper}: Numeric vector with lower boundaries of the covariates. The boundaries are needed since the dynamic \texttt{rf} adjustment \citep{corana_1987, pronzato_1984} depends on the number of invalid covariate combinations.
	\item \texttt{control}: A list with optional further parameters.
\end{itemize}
All parameters in the list with \texttt{control} arguments have a default value. They are pre-parameterized for loss functions of medium complexity. \texttt{control} arguments are:
\begin{itemize}
	\item \texttt{vf}: Variation function that allows the user to restrict the parameter space. This is one of the most important differences to classic SA. The function determines the variation of covariates during the iterations. It is allowed to depend on \texttt{rf}, \texttt{temperature} and the vector of covariates of the current iteration. The variation function is a crucial element of \texttt{optim\_sa} which enables flexible programming. It is (next to the loss function itself) the second possibility to define restrictions. The parameter space of the optimization program can be defined by \texttt{vf}. Per default, the covariates are changed by a continuous, uniformly distributed random number. It must be considered that defining specific \texttt{rf} can increase the calculation time. The default \texttt{rf} is a compiled C++ function whereas user specified \texttt{rf} must be defined as R functions. User specified \texttt{rf} are e.g. useful for optimization problems with non-continuous parameter space.
	\item \texttt{rf}: Numeric vector with random factors. The random factors determine the range of the random number in the variation function \texttt{vf} relative to the dimension of the function variables. The \texttt{rf} can be stated separately for each variable. Default is a vector of ones. If \texttt{dyn\_rf} is enabled, the entries in \texttt{rf} change dynamically over time.
	\item \texttt{dyn\_rf}: Boolean variable that indicates if the \texttt{rf} shall change dynamically over time to ensure increasing precision with increasing numbers of iterations. \texttt{rf} determines whether the adjustments of \citet{corana_1987} and \citet{pronzato_1984} are enabled (see method section for theoretical background). \texttt{dyn\_rf} ensures a relatively wide search domain at the beginning of the optimization process that shrinks over time. Disabling \texttt{dyn\_rf} can be useful when \texttt{rf} with high performance are known. The development of \texttt{rf} is documented in the \texttt{trace} matrix. Evaluation of former optimizations with dynamic \texttt{rf} can thus help finding efficient and reasonable fixed \texttt{rf}. Self-specified \texttt{vf} may not depend on \texttt{rf}. In this cases activating \texttt{dyn\_rf} will not have any advantage.
	\item \texttt{t0}: Initial temperature. The temperature directly influences the likelihood of accepting worse responses and thus the stochastic part of the optimization. \texttt{t0} should be adopted to the loss function complexity. Higher temperatures lead to a higher ability of coping with local optima but also to more time-consuming function calls.
	\item \texttt{t\_min}: Numeric value that determines the temperature where outer loop stops. As there is practically no chance of leaving local optima in iterations with low temperature \texttt{t\_min} mainly affects accuracy of the solution. Higher \texttt{t\_min} yields to lower accuracy and fewer function calls.
	\item \texttt{nlimit}: Integer value which determines the maximum number of inner loop iterations. If the break criterion in the inner loop is not fulfilled, \texttt{nlimit} is the exact number of inner loop repetitions. It is therefore an important parameter for determining the number of iterations.
	\item \texttt{r}: Numeric value that determines the reduction of the temperature at the end of each outer loop. Slower temperature reduction leads to an increasing number of function calls. It should be parameterized with respect to \texttt{nlimit}. High \texttt{nlimit} in combination with low \texttt{r} lead to many iterations with the same acceptance likelihood of worse responses. Low \texttt{nlimit} in combination with \texttt{r} near 1, by contrast, lead to a continuously decreasing acceptance likelihood of worse responses. It is thus the second crucial parameter for determining the number of iterations.
\end{itemize}

%%%%%%%%%%%%%%
%% Examples %%
%%%%%%%%%%%%%%
\section{Examples}
\label{sec:opt:examples}
To show the benefits of our optimization package, we build four examples explaining where the \texttt{optim\_sa} function can be advantageous.

%%-------------------%%
%% Himmelblau, cont. %%
%%-------------------%%
\subsection{Himmelblau Function with continuous parameter space}
\label{subsec:opt:examples:hbcont}
Himmelblau's function (Equation~\ref{eq:opt:eq2}) \citep{himmelblau_1972} was chosen as an initial example since it is a very simple multimodal equation and widely known in operations research. It has four equal minimum values ($min(f(x_1,x_2))=0)$) at \{-2.8, 3.1\}, \{3.0, 2.0\}, \{3.6, -1.8\} and \{-3.8, -3.3\}. In order to display the basic behavior of \texttt{optim\_sa}, it was compared with two other SA methods from the \textit{stats} package \citep{r_core_team_2016}. Himmelblau's function is relatively simple. Therefore we also included the NM optimization method \citep{nelder_1965} which is a default of \texttt{optim} from the stats package, to examine the advantages of stochastic search against direct search.

\begin{equation}
\label{eq:opt:eq2}
f(x_1,x_2)=(x_1^2+x_2-11)^2+(x_1+x_2^2-7)^2
\end{equation}

We performed 10,000 repetitions with each function in order to investigate the quality and speed of the solutions using parameters for relatively simple optimization problems for all examined methods. The optimizations were performed having the following parameters:
\begin{example}
# Example 5.1: Solving the Himmelblau Function (hi) with continuous parameter 
  space.

# stats package: optim (NM)
stats::optim(fn = hi, par = c(10, 10), method = "Nelder-Mead")
	
# optimization package: optim_sa
optimization::optim_sa(fun = hi, start = (c(10, 10)), trace = TRUE, 
	lower = c(-40, -40), upper = c(40, 40),
	control = list(t0 = 500, nlimit = 50, r = 0.85,
		rf = 3, ac_acc = 0.1, dyn_rf = TRUE))

# stats package: optim (SA)
stats::optim(fn = hi, par = c(10, 10), method = "SANN",
	control = list(tmax = 500, reltol = 0.1, temp = 50, trace = TRUE))
\end{example}
%% No line break
Since we have a multimodal optimization problem with multiple equal solutions, the evaluation of solutions quality is composed of response accuracy and covariate combination. With fixed starting parameters, the methods should be able to find all possible solutions. We parameterized the functions from the example such that thy fulfill a defined accuracy threshold. We defined the optimization problem to be solved properly when the responses of the 10,000 repetitions were in mean $\leq$ 0.01. We also looked at the frequency distribution of the covariate combination after minimization. Subsequent to the investigation of quality, we also compared the efficiencies by measuring calculation times using \textit{microbenchmark} \citep{mersmann_2015} and the iterations frequencies.

It became clear that parameterization of NM was quite simple. It only needed a vector with starting values. The other functions required a larger number of settings. With view to accuracy each method performed well. All functions returned in mean of 10,000 calls responses with values $\leq$ 0.01. Regarding frequency distribution, the functions performed differently (Table~\ref{tab:opt:tab1}). \texttt{optim\_sa} and \texttt{optim (SA)} returned all possible solutions. The combination \{-3.8, -3.3\} was consistently least frequent. As \texttt{optim (NM)} is a direct search method, it only returned one solution \{-3.8, -3.3\}. Further investigation revealed the solutions of \texttt{optim (NM)} to be sensitive to the starting values. If \texttt{optim (NM)} was parameterized with randomly drawn starting values, all four results would have been possible. Thus one advantage of \texttt{optim\_sa} and \texttt{optim (SA)} against direct search methods, like \texttt{optim (NM)}, is its independence from starting values. Random search methods are advantageous for solving multimodal optimization problems.

% latex table generated in R 3.2.3 by xtable 1.8-2 package - Modified after pasting
% Tue Mar 7 14:26:30 2017
\begin{table}[ht]
	\centering
	\caption{Relative frequencies of covariate combinations in \% after optimization of Example 5.1 for the four examined methods. Number of repetitions: 10,000. We used the parameters given in the example, only the \texttt{trace} options was deactivated.}
	\label{tab:opt:tab1}
	\begin{tabular}{cccccc} \cline{3-6}
		& \multicolumn{1}{c}{} & \multicolumn{4}{c}{result (rounded)}                    \\ \cline{3-6} 
		&                      & \{-2.8, 3.1\} & \{3.0, 2.0\} & \{3.6, -1.8\} & \{-3.8, -3.3\} \\ \cline{2-6} 
		\multirow{4}{*}{method} & optim\_sa          & 22.19     & 33.49    & 28.05     & 16.27      \\
		& optim (SA)            & 25.91     & 30.89    & 24.08     & 19.12      \\
		& optim (NM)          & 0.00      & 0.00     & 0.00      & 100.00     \\ \cline{2-6} 
	\end{tabular}
\end{table}


As all functions were practically able to minimize Equation~\ref{eq:opt:eq2}, comparison of calculation times appeared to be another important point for quality assessment. As expected, the direct search method \texttt{optim (NM)} was by far faster than all stochastic methods (Figure \ref{fig:opt:fig1}, left). The two functions able to cope with equally valued optima (\texttt{optim (SA)} and \texttt{optim\_sa}; Table~\ref{tab:opt:tab1}) were significantly slower (Figure~\ref{fig:opt:fig1}, left). We parameterized the functions such that they fulfill our accuracy requirements. The parameterizations in the examples should thus be near to the most efficient parameterizations for the given problem with the expected accuracy.

Another way of comparing algorithms' efficiencies is to examine the frequency of necessary iterations solving the optimization problem. Again, \texttt{optim (NM)} performed best (Figure~\ref{fig:opt:fig1}, right). \texttt{optim (SA)} required the most repetitions by far. Only \texttt{optim\_sa} showed varying frequencies because \texttt{optim\_sa} was, given the displayed parameters, the only function where the inner loop broke due to the threshold accepting criterion. \texttt{optim (SA)} always iterated till the user specified limit. It could be seen that \texttt{optim\_sa} performed well when compared to established SA based optimization function. \texttt{optim\_sa} is slower but it requires less iterations. The calculation of valid parameter combinations (Algorithm~\ref{alg:opt:alg1}, lines 7 to 15) leads to multiple function calls within one iteration. For this, one iteration of \texttt{optim\_sa} can last longer than one iteration of the other methods. The number of function calls is not necessarily equal to the number of iterations when \texttt{dyn\_rf} is activated. The \texttt{dyn\_rf} which adopts the search domain dynamically to specific loss functions on the one hand thus leads to longer calculation times on the other hand. Improved flexibility therefore again corresponds with longer calculation time.

To conclude, all functions were generally able to solve the problem. The flexible stochastic search-grid of \texttt{optim (SA)} and \texttt{optim\_sa} enabled archiving each of the four solutions. Practically, if users are in doubt whether a problem has multiple solutions with equal responses, \texttt{optim (SA)} and \texttt{optim\_sa} can simply be repeated without re-parameterization. If well-specified, they will return all possible solutions. They thus have advantages for complex multimodal problems of that kind but are also slower. Due to the multiple additional options, which all have to be called in the code, \texttt{optim\_sa} is slowest. This example clearly reveals that higher flexibility and generality leads to significantly higher calculation times. Specific algorithms have advantages for more complex problems while more general functions are useful for optimization problems with simple responses.

\textsc{\begin{figure}[htbp]
		\centering
		\resizebox{1.04\linewidth}{!}{\input{Grafiken/opt/fig1_ex1.tex}}
		\caption{Calculation times and frequencies of iterations of the four examined optimization algorithms in Example 5.1. Note that the y-axis of the left diagram is truncated for reasons of presentation. \texttt{optim\_sa} showed 101 and \texttt{optim (SA)} 70 outliers between four and seven milliseconds. The frequency of iterations represents the total number of iterations. Thus, for the SA methods, all inner loops repetitions are counted. The packages parameterization are shown in the example code.}
		\label{fig:opt:fig1}
	\end{figure}}

%%-------------------%%
%% Himmelblau, disc. %%
%%-------------------%%
\subsection{Himmelblau Function with discrete parameter space}
\label{subsec:opt:examples:hbdisc}
A second example illustrating the advantage of flexibly defining the parameter spaces also bases on the function by \citet{himmelblau_1972}. It is an exemplary optimization problem which cannot be solved with any other examined method. If a user is interested in integer covariate combinations only, simply rounding the solution is usually not satisfying since the solution could be non-feasible or non-optimal \citep{cardoso_1997}. The flexible variation function \texttt{vf} of our models allows searching for integer temporary solutions only and thus searching for the global integer solution. \texttt{vf} can be used to restrict the parameter space to the specific problem. The following simple example shows a variation rule for an exemplary integer programming problem. The simple \texttt{var\_fun\_int} from the following example returns a vector of integer covariates varying in dimension of \texttt{vf} around the passed covariates \texttt{para\_0}. The \texttt{fun\_length} must be passed explicitly, although it is implicitly given by the length of \texttt{para\_0} because it might help simplifying the function \texttt{vf}. Dependence of \texttt{rf} and \texttt{temp} can be implemented. Optimization problems do not always need a dynamic component. If \texttt{vf} need not be dynamic, i.e. if the stochastic part shall be fixed during the entire iterations, \texttt{temp} and \texttt{rf} can separately be disabled by passing \texttt{NA} for the respective parameter. If \texttt{vf} does not depend on \texttt{rf}, the option \texttt{dyn\_rf} will not be useful any more. It should be disabled in such cases to save computation time.

\begin{example}
# Example 5.2: Solving the Himmelblau Function with discrete parameter space.
# Define vf
var_fun_int <- function (para_0, fun_length, rf, temp = NA) {
	ret_var_fun <- para_0 + sample.int(rf, fun_length, replace = TRUE) *
		((rbinom(fun_length, 1, 0.5) * -2) + 1)
	return (ret_var_fun)
}
	
# Call optim_sa
int_programming <- optimization::optim_sa(fun = hi, start = c(10, 10),
	trace = TRUE, lower = c(-40, -40), upper=c(40, 40),
	control = list(t0 = 500, nlimit = 50, r = 0.85, rf = 3, 
		ac_acc = 0.1, dyn_rf = TRUE, vf = var_func_int))
\end{example}

Repeating the minimization function with the parameters from the example 10,000 times led to the only integer solution \{3, 2\} in 95.6 \% of the repetitions. The generic plot function of the optimization package (Figure~\ref{fig:opt:fig2}) helps interpreting the convergence and thus the solution quality as well as the algorithm behavior. Since the example is 2-dimensional, a \texttt{contour\_plot} (Figure~\ref{fig:opt:fig2}, right) could be created. It shows the state space of the Himmelblau function in continuously changing colors for parameter spaces between -4 and 4. The results at the end of each outer loop iteration are shown as points within the parameter space. It became obvious that only integer covariate combinations were calculated during the entire optimization (Figure~\ref{fig:opt:fig2}, right). Figure~\ref{fig:opt:fig2} additionally helps explaining the stochastic part of the SA method. It becomes clear that though the response of iteration 10 (parameter combination \{0, 0\}) was already quite near 0, following iteration ended with relatively worse intermediate results. With response > 100, iteration 19 (parameter combination \{2, -3\}) was much worse than iteration 10. The likelihood of keeping worse solutions shrank over time till it became practically 0 after iteration 40 (Figure~\ref{fig:opt:fig2}, left). For problems of such kind, 40 iterations therefore should be far enough. This information could help the user parameterizing the function for the next problem of such kind.

\texttt{optim (SA)} also provides possibility of defining a variation function via the \texttt{gr} statement. Using the variation function from the example code with fixed \texttt{rf}, however, it was not possible to define robust parameters for solving an integer programming problem. \texttt{optim\_sa} is thus advantageous against all other examined functions when the parameter space of the optimization problem underlies extended restrictions. It is the only function that enables global optimum integer search. To achieve this, the user must define a changing rule in form of a R function. Users must thus examine their optimization problem very carefully and translate it into suitable functions. Parameterization of \texttt{optim\_sa} is quite complex and time extensive but enables adopting the optimizer to problem specific needs. Other restrictions such as mixed integer problems may be defined analogously. \texttt{vf} can thus be used to flexibly restrict the parameter space. Further suggestions for interpretation can be found in the package documentation.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.025\textwidth]{Grafiken/opt/fig2-ex2-plot.eps}
	\caption{Exemplary examination plots created with the generic plot function. The left diagram shows the current optimal response over iteration of the outer loop. The left diagram displays the succession of the covariate values. The star points the covariate combination at optimum. The actual parameter space of the optimization is reduced for presentation purposes.}
	\label{fig:opt:fig2}
\end{figure}

%%--------------%%
%% SVAR Example %%
%%--------------%%
\subsection{Structural vector autoregressive models with least dependent innovations}
\label{subsec:opt:examples:svar}
\textit{Vector autoregressive} (VAR) models are used to examine dynamic structures of several time series endogenously within one model. Usually, impulse response functions come to use for investigating structural shocks in those multivariate time series systems. Those impulse responses need a decomposition of the covariance matrix. The covariance matrix decomposition, however, is not unique. The interaction between the time series variables in a VAR model are not directly calculable without further assumptions or information. The idea behind \textit{structural} VAR (SVAR) is to define strategies to overcome this problem and enable unique impulse response definitions \citep{lutkepohl_2006}.

Least dependent innovations is one of plenty possibilities to obtain unique decompositions of the covariance matrix. The idea is to minimize a Cramer-von-Mises test statistic for stochastically independent structural errors \citep{genest_2007}. Basically, the structural errors are multiplied with $K(K-1)/2$ Givens rotation matrices with angles $\theta$ between \textit{0} and $2\pi$, until the test statistic from equation \ref{eq:opt:cvm} becomes minimal. \textit{K} is the number of time series in the SVAR model, \textit{T} the number of observations and $\tilde{\varepsilon}$ are the rotated structural errors. An advantage of this method is, that it is very liberal in sense of distribution assumptions \citep{herwartz_2014}.
\begin{equation}\label{eq:opt:cvm}
\mathcal{B}_\theta = \int_{(0,1)^K}\left[\sqrt{T}\left(C(\tilde{\varepsilon}) - \prod^K_{i = 1}U(\tilde{\varepsilon}_i)\right)\right]^2d\tilde{\varepsilon}
\end{equation}
However, optimizing the test statistic is challenging, because the loss function has a large and unpredictable number of local optima and an irregular state space. Moreover, the function shows a relatively broad parameter space with very sensitive responses. It often reacts dramatically to small changes in the rotation angles. One approach to solve the optimization problem is the \textit{grid optimization} \citep{herwartz_2015}. Grid optimization is, however, inefficient in large samples and -- depending on the step size -- sometimes unable to find the global optimum. Monte Carlo simulations have shown that direct search and gradient based optimization algorithms usually fail to find the optimal rotation angles. Performing several direct search optimizations sequentially with randomly changing starting values can be lead to satisfying solutions. The strategy is, however, also very inefficient.

The irregular response pattern and the high demands on the solution precision are the main challenges. The combination of SA with the adaptive variation of the search seems to be promising for the optimization problem. The relatively broad parameter variation at the beginning of the optimization process should ensure adequate search-grid size while the stochastic component of the SA should be able to tackle the irregular responses. Dynamically decreasing parameter variation is particular advantageous for optimizing the rotation angles as sufficient parameter accuracy is crucial for the reliability of the solution.

Practical application of \texttt{optim\_sa} revealed its advantages. Using parameterizations for low performance and high precision, like an initial temperature of 100,000, a maximum number of 10,000 inner loops and temperature reductions between 0.7 and 0.9, still converged faster than all other investigated methods. Sensitivity analysis of exemplary optimization problems confirmed the general applicability of \texttt{optim\_sa}. The function was able to find the actual global optimum sufficiently often.
%%-----------------------------------------%%
%% Forest harvesting schedule optimization %%
%%-----------------------------------------%%
\subsection{Forest harvesting schedule optimization}
\label{subsec:opt:examples:forest}
Forestry is traditionally a knowledge-based field with optimization playing only a minor role. However, optimization methods are very interesting for intermediate-term forest planning. Their popularity is increasing \citep{mohring_2010}. While Linear Programs are nowadays used in some states, e.g. Finland \citep{redsven_2012}, stochastic optimization programs are quite novel in forestry for optimization of harvesting intensity \citep{kangas_2015}. Our function is an integral part of the first stochastic optimization software of forest operation in Germany and one of the first software solutions worldwide on single tree scale. Optimization of forest harvesting planning represents an interesting and innovative practical example where \texttt{optim\_sa} is recently used. The \texttt{optim\_sa} function is part a innovative forest development simulation-optimization software for decision support of forest enterprises. The software is an optimization add-on for the widely-used forest development simulation software \textit{WaldPlaner} by \citet{hansen_2014}. WaldPlaner is a user front end for \textit{Tree Growth Open Source Software} (\textit{TreeGrOSS}), which is a complex Java written tree growth and yield simulation software used to forecast the developments of forest management areas (forest stands) developed by \citet{nagel_1996}. It is a tool able to simulate forest enterprises with hundreds of forest stands simultaneously where the smallest simulation element is the single tree. Each tree in the system is simulated individually. Optimization of forest activities is, accordingly, not trivial since TreeGrOSS is a complex network of rules and functions which are predominantly nonlinear. The entire optimization process is composed of TreeGrOSS, \texttt{optim\_sa}, an interface function to enable communication between TreeGrOSS and the optimizer as well as a data warehouse. This multi-factorial composition implies high demand for flexibility of the optimization function. The loss function, which represents in this example the interface between the three elements of the optimization system, must be composed of R, Java (using \textit{rJava}; \citealp{urbanek_2016}) and SQL (using \textit{RPostgreSQL}; \citealp{conway_2016}). Main tasks of the interface are enabling communication between TreeGrOSS and optimization algorithm and rating the TreeGrOSS output in terms of costs and revenue such that the TreeGrOSS output is translated into a response readable by the optimization algorithm. Flexibility of loss and variation functions is hence a prerequisite for forest harvesting schedule optimization via TreeGrOSS simulations. Each loss function call causes a TreeGrOSS simulation and a database operation. In order to save time, parts of the loss are therefore programmed parallel. The concept of sustainability plays a central role in forestry. Forests must be treated such that recent and further generations can benefit from their economic, ecologic and social functions. Particular harvesting operations must not exceed the sustainable wood growth. Each function call returns, next to the actual response, a sustainability index. This index is used to restrict the optimization by returning \texttt{NA} responses whenever sustainability is violated. Sustainability is hence an important restriction forest schedule optimization that further increases complexity of the loss function as the loss partially returns invalid values.

The example again reveals the complexity of the problem and further reinforces the need for specific stochastic methods. Only \texttt{optim\_sa} was able to solve the complex loss function of the forest harvesting schedule problem. Sensitivity analysis using an exemplary forest enterprise comprised of five forest sites, with known global maximum, reinforced reliability of \texttt{optim\_sa} for harvesting optimization. A solution sufficiently near the global maximum was found in arguable time on a standard personal computer. To test the practical usability, the optimization system was additionally tested on a real forest enterprise with 100 forest sites. The problem was solved using a high performance cluster computer. The simulation-optimization approach calculated reasonable and reliable solutions.

%%%%%%%%%%%%%%%%
%% Discussion %%
%%%%%%%%%%%%%%%%
\section{Discussion and outlook}
\label{sec:opt:discussion}
In conclusion, SA methods have considerable advantages over classical optimization procedures if the loss function is non-linear, multimodal and partially undefined. Our simulated annealing approach optimizes these functions quite well. It might not be faster than the standard simulated annealing approach of the stats package \citep{r_core_team_2016}, but that was not its purpose. However, due to the shrinkage of the parameter space and the extra stopping criteria it manages to require fewer steps than the standard approach. Additionally, the main advantage of our new approach is to deal with irregular parameter spaces, as well as with non-continuous and sophisticated loss functions, where the standard approach reaches its limits. In our examples we show that problems with these characteristics exist and that our algorithm solves these problems. Furthermore there are several examples of loss functions in natural science where some combinations of variables are restricted and our approach shows its benefits.

%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements %%
%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
\label{sec:opt:acknowledgements}
The authors are grateful to the Project Management Jülich of the Federal Ministry of Education and Research for funding this research through the Cluster \textit{Bio-Economy} (031A294). We thank Michael Hill for reviewing the language of the manuscript.
\newpage

%%%%%%%%%%%%%%
%% Appendix %%
%%%%%%%%%%%%%%
\section{Appendix: Case studies of the combined simulation-optimization approach}
\label{sec:opt:appendix}
The following section provides supplemental information about the development and the interpretation of the forest harvesting schedule optimization example from the essay (Subsection \ref{subsec:opt:examples:forest}). In the essay above, the statistical and technical foundations of the optimization module (Figure \ref{fig:Introduction:flowopt}) are introduced. The high demands of forest harvesting schedule optimization on the optimization module are illustrated and discussed in Subsections \ref{subsec:intro:struct:opt} and \ref{subsec:opt:examples:forest}. A Simulated-Annealing-based optimizer, specifically developed for the usage in the simulation-optimization software, is presented.

The applicability of the software package for optimization of forest harvesting is briefly confirmed in two practical examples (Subsection \ref{subsec:opt:examples:forest}). In the following, these two examples are described in more detail in order to facilitate the discussion and the findings from the examples with view to the research question (Section \ref{sec:intro:aim}).

\subsection{Motivation}
\label{subsec:opt:appendix:motivation}
To explain its basic behavior and to confirm its applicability in practice, the si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software were performed on two exemplary forest enterprises (Subsection \ref{subsec:opt:examples:forest}). These two enterprises were generated to explain and discuss how the si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software can be used to support decisions in the intermediate-term forest planning. The first exemplary enterprise was used to show the basic behavior of the si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software. It was comprised of five forest stands which were specifically chosen for exemplary purposes. The second exemplary enterprise was composed such that it represented a typical forest enterprise of central Germany covering all relevant age classes of European beech for the growing bio-economy sector. The second example explained the reasonability of the si\-mu\-la\-tion-op\-ti\-mi\-za\-tion approach for typical forest enterprises of the most important region for the supply of the wood-based bio-economy industry (Section \ref{sec:hzb:Einleitung}).

Three simulation scenarios were calculated for each exemplary enterprise. The stand developments were forecasted for 20 years using three different simulation approaches. The forest developments and the wood potentials of both exemplary enterprises were firstly forecasted without any optimization using the TreeGrOSS forest growth and yield simulation packages with standard treatment parameters (see Section \ref{sec:intro:dss} for a brief introduction of TreeGrOSS). The stand developments under standard regimes were compared with the optimized treatment simulations of those stands. Accordingly, in the second scenario, the harvesting intensities were optimized using the combined si\-mu\-la\-tion-op\-ti\-mi\-za\-tion approach (Figure \ref{fig:Introduction:flowopt}). In the second scenario, the maximized intermediate-term wood potential of the exemplary forest enterprises was calculated. The resulting wood volume locations and quantities display the maximal achievable wood potential under the given assumptions and restrictions. Binding delivery contracts can be advantageous for forest enterprises as well as for wood processing companies. They can, however, reduce the enterprise-specific intermediate-term wood potential (Subsection \ref{subsec:intro:struct:opt}). For this, in the third scenario, annual minimum harvesting volumes restrictions were implemented into the optimization system. The third scenario was performed to explain how the si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software could be used to examine the drawbacks of delivery contracts with binding wood amounts. It is explained how decision-makers of forestry and bio-economy could use the si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software to find a trade-off between the advantages and the drawbacks of delivery contracts or any other reason that forces forest enterprises to harvest annual minimum wood amounts.

\subsection{Scenario definitions}
\label{subsec:opt:appendix:method}
All selected stands of both exemplary enterprises were located in the public forest district of Reinhausen \citep{nlf_2017}. Using the WaldPlaner import plug-in \citep[p. 58]{hansen_2014}, the official inventory data of Reinhausen were used to generate TreeGrOSS forest simulation stands. The stands were directly stored into a PostgreSQL data-warehouse \citep{eisentraut_2003} via the import plug-in. The inventory data were originally collected for the intermediate-term forest planning in Reinhausen (see Section \ref{sec:intro:dss} for further details of the intermediate-term forest planning process). The simulated stands of both exemplary forest enterprises thus based on real forest stands of southern Lower Saxony.

The five stands of the first enterprise were consciously chosen such that they cover the most relevant age classes for the growing bio-economy sector (Table \ref{tab:discussion:struct:opt:application:tab1}). Since the simulation-optimization software was programmed to support planning between the forestry and the bio-economy sector and since the wood of European beech is one of the most interesting raw materials for the upcoming wood-based bio-economy sector, the first exemplary forest enterprise was composed of European beech stands with little share of spruce. It was shown before that wood potentials for the bio-economy sector can be found particularly in smaller wood dimensions (Chapter \ref{chap:hzb}). The first exemplary enterprise was therefore compiled of intermediate-aged stands with average stand diameters (diameter of stem of mean basal area) below 40 cm. Under the given scenario definitions (Table \ref{tab:discussion:struct:opt:application:tab2}), high-valued stem wood from target diameter usage was therefore not expectable during the simulation period of 20 years.


\begin{table}[h]
	\centering
	\caption{Stand specific attributes of the first exemplary forest enterprise at the date of the forest inventory (2011). The parameters were generated with the stand summary function of WaldPlaner \citep[p. 64]{hansen_2014}. dm: Diameter of stem of mean basal area. hm: Height of stem of mean basal area.}
	\label{tab:discussion:struct:opt:application:tab1}
	\begin{tabular}{cccccccc}
		\hline
		stand              & species & 
		\begin{tabular}[c]{@{}c@{}}share\\ 	{[}\%{]} \end{tabular}
		
		& \begin{tabular}[c]{@{}c@{}} mean age\\ {[}a{]} \end{tabular} & 
		\begin{tabular}[c]{@{}c@{}} dm\\ {[}cm{]} \end{tabular} &
		\begin{tabular}[c]{@{}c@{}} hm\\ {[}m{]} \end{tabular} &  \begin{tabular}[c]{@{}c@{}}stand volume\\ {[}m$^3$ ha$^{-1}${]}\end{tabular} & \begin{tabular}[c]{@{}c@{}} stand size\\ {[}ha{]} \end{tabular} \\ \hline
		1                  & beech   & 100            & 52               & 14          & 21.3        & 227  & 1.4                                                                \\
		2                  & beech   & 100            & 62               & 16          & 21.0        & 269  &0.2                                                                \\
		3                  & beech   & 100            & 102              & 30          & 28.3        & 349                                                        & 0.75          \\
		4                  & beech   & 100            & 109              & 34          & 30.2       & 403                                                          & 0.4        \\
		
		\multirow{2}{*}{5} & beech   & 66  & 69   & 18   & 22.9 & 113 & \multirow{2}{*}{0.2}      \\
		& spruce  & 33  & 61    & 22  & 26.6 & 233     \\
		
		\hline
	\end{tabular}
\end{table}

The second forest enterprise was comprised of 100 forest stands covering 560 ha of forest land in total. The stands of the second enterprise were randomly chosen from a population of 994 forest stands. The population covered all stands of Reinhausen with a share of European beech above 10 \%. Reinhausen was chosen as the population for the random selection since its species and age mixture was similar to the characteristic mixture of central Germany (Figure \ref{fig:hzb:fig3_altersklassen}; \citealp{nlf_2017}). The second exemplary forest enterprise therefore represented a typical forest enterprise of central Germany in terms of the enterprise size (Subsection \ref{subsec:hzb:Ergebnisse:Waldeigentum}), the tree species mixture and tree age configuration (Subsection \ref{subsec:hzb:Ergebnisse:Alter}). All relevant wood dimensions for the wood-based bio-economy were covered in the example. Figure \ref{fig:discussion:fig2} (the left bar) shows the volume distribution of the tree species at the date of the forest inventory (2011).

\begin{table}[h]
	\centering
	\caption{Simulation settings of the standard treatment. For detailed listings of the specific tree species within the species groups see \citet[p. 193-195]{hansen_2014}. dll: deciduous tree species with a long life expectancy. dsl: deciduous tree species with a short life expectancy}
	\label{tab:discussion:struct:opt:application:tab2}
	\begin{tabular}{cccc}
		\hline
		species group & \begin{tabular}[c]{@{}c@{}}target diameter\\ {[}cm{]}\end{tabular} & \begin{tabular}[c]{@{}c@{}}tree height at\\initial thinning\\ {[}m{]}\end{tabular} & \begin{tabular}[c]{@{}c@{}}number of future\\crop trees\\ {[}N ha$^{-1}${]}\end{tabular} \\ \hline
		oak         & 80 &14     & 80     \\
		European beech         & 60                                                                 & 16                                                                                  & 100                                                                                \\
		dll           & 60                                                                 & 12                                                                                  & 80                                                                                 \\
		dsl           & 35-45                                                              & 10                                                                                  & 80                                                                                 \\
		spruce        & 45                                                                 & 12                                                                                  & 200                                                                                \\
		douglas fir     & 65                                                                 & 14                                                                                  & 120                                                                                \\
		pine        & 45                                                                 & 13                                                                                  & 180                                                                                \\
		larch        & 60                                                                 & 12                                                                                  & 120                                                                                \\ \hline
	\end{tabular}
\end{table}

To assess the sensitivity of the simulation-optimization approach, three scenarios were calculated for each forest enterprise (Table \ref{tab:discussion:struct:opt:application:scenarios}). Firstly, standard treatment settings were used to forecast the usual growth and yield of the two exemplary enterprises for 20 years using the TreeGrOSS packages (Table \ref{tab:discussion:struct:opt:application:scenarios}, scenario 1). Scenario 1 was a determined forest growth and yield simulation without any optimization. Detailed information about the possibilities of parameterizing forest growth and yield simulations with TreeGrOSS are given in \citet[p. 90-93]{hansen_2012}. The treatment options, which were used to parameterize the simulation via TreeGrOSS, were developed by a work group of forest scientists and practitioners in order to investigate current and future wood potentials in central Germany (Chapter \ref{chap:hzb}). These simulation settings represented the typical forest treatments in central Germany. Table \ref{tab:discussion:struct:opt:application:tab2} gives an overview of the crucial settings. Scenario 1 hence reflected the standard forest development without treatment optimization via the simulation module of the simulation-optimization software (Figure \ref{fig:Introduction:flowopt}).

The si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software requires a reference scenario which builds the basis for the optimization (Subsection \ref{subsec:intro:struct:opt}). This reference scenario must be a set of TreeGrOSS settings which are iteratively changed by the optimization module of the si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software (Subsection \ref{subsec:intro:struct:opt}; Figure \ref{fig:Introduction:flowopt}). To make scenarios 1 and 2 directly comparable, the standard treatment settings from scenario 1 were chosen as reference for the optimization. Scenario 2 was actually an optimization of the harvesting intensity settings that were developed for the forecasting of the wood potential in central Germany (Table \ref{tab:discussion:struct:opt:application:scenarios}). The optimization in scenario 2 was performed without delivery restrictions in order to assess the maximal possible wood potential for 20 years. It was the maximal achievable wood potential without violating the specific implementation of the sustainability principle and the enterprise-specific strategical orientation (see Subsection \ref{subsec:intro:struct:opt} for methodological details).

In scenario 3 (Table \ref{tab:discussion:struct:opt:application:scenarios}), minimum delivery restrictions for low-dimensioned deciduous wood from thinning activities were defined. The methodological details are explained in Subsection \ref{subsec:intro:struct:opt}. Those minimum delivery restrictions were defined to simulate continuous delivery restrictions between companies from the bio-economy sector and forest enterprises. Despite these minimum harvesting restrictions, all settings were similar to scenario 2. The restrictions amounted to 4 m$^3$ year$^{-1}$ (20 m$^3$ per 5-year period) for the first, and to 900 m$^3$ year$^{-1}$ (4,500 m$^3$ per 5-year period) for the second exemplary forest enterprise.

\begin{table}[ht]
	\centering
		\caption{Overview of the three scenarios that were developed for sensitivity analysis of the simulation-simulation software.}
		\label{tab:discussion:struct:opt:application:scenarios}
		\begin{tabular}{ccc}
			\hline
			scenario & treatment & restictions \\ 
			\hline
			1 & \begin{tabular}{c}standard, determined\end{tabular} & no restrictions \\ 
			2 & optimized & no restrictions \\ 
			3 & optimized & \begin{tabular}{c}annual minimum delivery amounts\\for low-valued deciduous wood\end{tabular}
			\\
			\hline
		\end{tabular}
\end{table}

\subsection{Detailed results}
\label{subsec:opt:appendix:results}
To enable a differentiated discussion of the findings from the case studies, the results of the case studies are explained in detail in the following paragraphs.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[ht]
	\centering
	\caption{Stand volumes of the five forest stands from the first exemplary forest enterprise at the end of the simulated scenarios (year 2031).}
	\label{tab:discussion:struct:opt:application:vol_after}
	\begin{tabular}{ccccc}
		\cline{1-1} \cline{3-5}
		\multirow{2}{*}{stand} &  & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}stand volume\\ {[}m$^3$ ha$^{-1}${]}\end{tabular}} \\ \cline{3-5} 
		&  & scenario 1               & scenario  2              & scenario 3              \\ \cline{1-1} \cline{3-5}
		1   &  & 462           & 464          & 441         \\
		2   &  & 434           & 432          & 417         \\
		3   &  & 534           & 529          & 534         \\
		4   &  & 588           & 542          & 596         \\
		5   &  & 559           & 569          & 555         \\ \cline{1-1} \cline{3-5} 
	\end{tabular}
\end{table}
As the first exemplary enterprise was composed of five stands only, the development of each stand was recorded in detail. Table \ref{tab:discussion:struct:opt:application:tab1} shows the stand volumes at the beginning, Table \ref{tab:discussion:struct:opt:application:vol_after} the stand volumes at the end of the simulations. The stand volumes of the standard treatment simulation (scenario 1) built the predefined limit volumes for the optimization scenarios (see Subsection \ref{subsec:intro:struct:opt} for methodological details). It was shown that all stand volumes of scenario~1 increased. This development reflects the typical wood usage below the average wood growth in intermediate-ages European beech stands (Figure \ref{fig:hzb:fig4_zuw_nutz}). It was the expectable development under the given species and age mixture.

The si\-mu\-la\-tion-op\-ti\-mi\-za\-tion software was programmed such that the sustainability limitations base on the stand volumes of scenario 1. Accordingly, the stand volumes in the last simulated year of the second scenario were expected to be similar. Deviations from the stand volumes of the standard scenario are only allowed in very narrow intervals. Table \ref{tab:discussion:struct:opt:application:vol_after} reveals the fulfillment of this implemented sustainability restriction. It was shown that each stand volume at the end of scenario 2 differed only slightly from the stand volumes at the end of the standard simulation. Considerable differences were only observed in the stands 4 and 5. The sum of the stand volumes of all 5 stands differed only very slightly between scenario 1 and 2 (1.6 \%).

\textsc{\begin{figure}[h]
		\center
		\resizebox{1\linewidth}{!}{\input{Grafiken/discussion/out_reinhausen_5.tex}}
		\caption{Summed simulated yields of the first exemplary forest enterprise in five-year periods, beginning with the period from 2011 to 2016. The simulated yields are differentiated into coniferous and deciduous wood volume (including bark).}
		\label{fig:discussion:fig1}
	\end{figure}
}
The simulated yields, though, differed substantially between scenario 1 and 2 (Figure \ref{fig:discussion:fig1}). Yields from target usage were negligible in all scenarios of the first exemplary enterprise. It was observed that intensive harvesting in the second time period was favorable in terms of total yields. The optimal enterprise-specific wood potential within the simulation period would be achieved, if the stands were treated intensively between 2016 and 2021. With an overall yield of 267 m$^3$, the wood potential of the optimized treatment scenario was 32 \% higher than the potential of the standard scenario. Extremely unbalanced stand treatments performed best in terms of the total wood potential.

Cooperations between forest enterprises and bio-economy companies can involve agreements about continuous wood supply amounts (Subsection \ref{subsec:intro:struct:opt}). Furthermore, forest owners themselves could be interested in balanced incomes from wood usage as they usually have to tackle continuous fixed costs like labor costs or repayments of loans \citep[p. 74]{mohring_2010b}. So there are good reasons why forest enterprises and bio-economy companies could be interested in balanced yields. This was in contrast with the results from the scenarios 1 and 2 because both scenarios forecasted only little wood potentials in at least one time period. If delivery contracts with balanced annual wood amounts were arranged, minimum harvesting restrictions would have been indispensable in the presented example. For this, minimum harvesting restriction for low-valued European beech wood from thinning activities were defined (Table \ref{tab:discussion:struct:opt:application:scenarios}, scenario 3). The restriction amounted to 4 m$^3$ year$^{-1}$ (20 m$^3$ per 5-year period). Due to the implemented sustainability restrictions, the stand volumes after simulation of scenario 3 were similar to the stand volumes after simulation of scenario 1 (Table \ref{tab:discussion:struct:opt:application:vol_after}), just as it was observed for scenario 2. The minimum harvesting restrictions had substantial influence on the stands treatments. Figure \ref{fig:discussion:fig1} (right) reveals that the minimum restriction led to homogeneous yields for deciduous wood. The deciduous harvesting volumes in the first time period, which was below the minimum restriction in scenarios 1 and 2, amounted roughly to the minimum restriction (20,7 m$^3$ > 20,0 m$^3$) in the third scenario. This reveals that usage in the first 5-year period is, under given tree species and age mixture, not reasonable in terms of wood potential optimization. The restrictions force a usage in this period. Tree harvesting in period one was, despite a few trees, ahead of the reasonable harvesting schedule. The tree-individual growth potential would not be used, if the trees were used in the first period. A force to harvest them, of course, led to a reduction of the overall wood potential. The summed wood potential of scenario 3 was 17 \% below the potential of the unrestricted scenario 2. This difference in the wood potentials built the basis for the calculation of the opportunity costs. Opportunity costs analysis enables a differentiated, objective trade-off between advantages and drawbacks of delivery contracts or any other reason that implies continuous minimum wood amounts. In the example, the matching of continuous wood demands of 4 m$^3$ year$^{-1}$ would reduce the total available wood potential in the 20-year simulation period by 40 m$^3$. Forest owners could use the results to decide whether the advantages of balanced yields justified opportunity costs of 40 m$^3$.

\textsc{\begin{figure}[ht]
		\center
		\resizebox{.6\linewidth}{!}{\input{Grafiken/discussion/standard_development_100.tex}}
		\caption{Development of the summed stand volumes of all forest stands of the second exemplary forest enterprise. The forest stands were forecasted using the standard treatment settings without optimization (scenario 1). dll: deciduous tree species with a long life expectancy. dsl: deciduous tree species with a short life expectancy.
		}
		\label{fig:discussion:fig2}
	\end{figure}
}
As the second exemplary enterprise contained too many stands to evaluate each stand volume separately, the stand volumes were aggregated into sums of tree species groups. Only the aggregated volume developments of the standard scenario (1) are shown in Figure \ref{fig:discussion:fig2}. Further investigation revealed that the stand volumes at the end of the simulations were, just as it was observed for the first exemplary enterprise, very close. The sustainability module hence also worked in the second example. The tree scenarios differed, again, in the allocations of the yields but not in the stand volumes at the end of the simulations.

It showed that the growth, by trend, exceeded the usage in scenario 1. The stand volumes were therefore growing during the simulation. Especially for European beech, the summed stand volumes increased till 2026 (Figure \ref{fig:discussion:fig2}). This reflects the typical development of forest stands in the forest district of Reinhausen \citep{nlf_2017} as most of the European beech stands were in age classes below 120 years at the beginning of the simulations.

The results of the second exemplary enterprise (Figure \ref{fig:discussion:fig3}) were not as obvious as the results of the first example (Figure \ref{fig:discussion:fig1}). With an overall yield of 78,500 m$^3$, the optimized treatment (scenario 1) simulation led to 9 \% higher harvesting amounts than the standard treatment simulation. Only the potentials of deciduous thinning assortments in the third and fourth time periods differed substantially between the scenarios. Analogously to the first exemplary enterprise, the third simulated scenario was parameterized with the same options as the second scenario despite the annual minimum harvesting volumes (Table \ref{tab:discussion:struct:opt:application:scenarios}). The evaluation of the scenario with minimum harvesting volumes for deciduous thinning assortments (scenario 3) uncovered mainly two results. The first result was that the differences in the overall wood potential were relatively slight between the scenarios 2 and 3 (about 1.5 \%). Secondly, the minimum harvesting amount for deciduous wood in scenario 2 (observed in the first time period) was already very close to the restricted harvesting amount of scenario 3. The simulation with optimized treatment intensity nearly forecasted the continuously available wood potential of the second exemplary enterprise. Minimum harvesting amounts that were substantially higher than the actually forecasted amounts of scenario 2 were not possible without violating the implemented sustainability definitions. Defining minimum harvesting amounts for deciduous thinning assortments above 4,500 m$^3$ in five years (900 m$^3$ year$^{-1}$) did not lead to valid results. This means that the minimum harvesting restriction definition of scenario 3 represented the limit of the continuously available wood potential of the second exemplary enterprise. Contracts with binding delivery wood volumes above 900 m$^3$ year$^{-1}$ for deciduous low-valued wood were not possible under the given species tree species and age mixture. For comparison, the actual minimal harvesting volume in scenario 2, observed in the first time period, was 838 m$^3$ year$^{-1}$. The only remarkable differences between scenarios 2 and 3 were found in the allocations of the yields in the time periods three and four. These allocations influenced the overall wood potential only slightly. The opportunity costs, calculated as the difference in the overall wood volumes between the restricted scenario (3) and the unrestricted scenario (2), amounted to 1,200 m$^3$. Delivery contracts with continuous wood amounts that were 60 m$^3$ year$^{-1}$ higher than the optimized amounts would hence cause a reduction of the overall wood potential of about 1,200 m$^3$.

With view to the mean stand ages in the forest district of Reinhausen at the date of the inventory, the calculated results were not surprising. Further investigation of the inventory data revealed that most of the European beech stands were in age classes below 120 years at the beginning of the simulations. The harvestable wood potential is typically below the actual growth in forest stands of such age classes to ensure a stand development with all relevant age classes being covered. In order to facilitate future wood potentials in high-valued stem timber assortments, growth must exceed the usage in younger and intermediate-aged European beech stands being younger than 120 years (Subsection \ref{subsec:hzb:Ergebnisse:Nachhaltig}). Higher usages in younger and intermediate ages stands would lead to a reduction of the future wood potentials in high-valued stem wood assortments. The limitations in the continuously available wood potentials in the first 5-year period (Figure \ref{fig:discussion:fig3}) were hence caused by the tree age configuration in Reinhausen. The continuously viable wood volumes in the following simulated periods would be much higher.

\textsc{\begin{figure}[h]
		\center
		\resizebox{1\linewidth}{!}{\input{Grafiken/discussion/out_reinhausen_100.tex}}
		\caption{Summed simulated yields of the second exemplary forest enterprise in 5-year periods, beginning with the period from 2011 to 2016. The simulated yields are differentiated into coniferous and deciduous as well as in thinning and target usage wood volume (including bark). The goal diameters for the target usages are given in Table \ref{tab:discussion:struct:opt:application:tab2}.}
		\label{fig:discussion:fig3}
	\end{figure}
}